{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "63e8aad6-28c5-48bb-a723-c9236a3bf3cf",
   "metadata": {},
   "source": [
    "# Part 01: Feature Backfill for **Spotify Sweden Daily Top 200**\n",
    "\n",
    "## Goal\n",
    "- Backfill daily chart observations (Top 200, region = Sweden)\n",
    "- Enrich with Spotify Web API metadata (optional)\n",
    "- Validate the dataset\n",
    "- Push to **Hopsworks Feature Store** as a time-travel enabled Feature Group\n",
    "\n",
    "✅ Output Feature Group example: `spotify_se_daily_top200_v1`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cadc0b5-0e26-445b-b57e-9142e70e5455",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0a9e34f1-11e6-492e-823e-f2c5cd7c9741",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading .env from: C:\\Users\\lppap\\Documents\\master\\scalable_ML\\ID2223-Final-Project-Spotify\\.env\n"
     ]
    }
   ],
   "source": [
    "# If you run this in a clean environment, you may need:\n",
    "# !pip install pandas requests tqdm python-dotenv hopsworks spotipy great-expectations\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import time\n",
    "import math\n",
    "import lxml\n",
    "import requests\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "from datetime import datetime, timedelta, date\n",
    "\n",
    "root_dir = Path().absolute()\n",
    "if root_dir.parts[-1:] == ('spotify',):\n",
    "        root_dir = Path(*root_dir.parts[:-1])\n",
    "if root_dir.name == \"notebooks\":\n",
    "    root_dir = root_dir.parent\n",
    "\n",
    "env_path = root_dir / \".env\"\n",
    "print(\"Loading .env from:\", env_path)\n",
    "\n",
    "load_dotenv(env_path)\n",
    "\n",
    "pd.set_option(\"display.max_columns\", 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1cafb345-a69a-460d-9c8a-9868226a0f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: PASS TO ENV\n",
    "\n",
    "# Region and chart type\n",
    "REGION = \"se\"          # Sweden\n",
    "FREQUENCY = \"daily\"    # daily charts\n",
    "CHART = \"regional\"     # Top 200 chart pages on spotifycharts.com\n",
    "\n",
    "# Backfill window\n",
    "START_DATE = date(2024, 1, 1)\n",
    "END_DATE   = date(2024, 3, 31)   # inclusive in our loop\n",
    "\n",
    "# Safety: top N rows (Top 200)\n",
    "TOP_N = 200\n",
    "\n",
    "# Optional: enrich with Spotify Web API\n",
    "ENRICH_WITH_SPOTIFY_API = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddb56765-366c-4acd-b143-ef758b0030ba",
   "metadata": {},
   "source": [
    "## Validate required secrets / environment variables\n",
    "\n",
    "You can store secrets locally in a `.env` file or in Hopsworks Secrets.\n",
    "\n",
    "**Needed if you enable enrichment**:\n",
    "- `SPOTIFY_CLIENT_ID`\n",
    "- `SPOTIFY_CLIENT_SECRET`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2543ca64-62c1-443f-8821-29692159e9e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Environment looks OK (or enrichment disabled).\n"
     ]
    }
   ],
   "source": [
    "def require_env(var_name: str) -> str:\n",
    "    val = os.getenv(var_name)\n",
    "    if not val:\n",
    "        raise ValueError(f\"Missing environment variable: {var_name}\")\n",
    "    return val\n",
    "\n",
    "if ENRICH_WITH_SPOTIFY_API:\n",
    "    # These must exist for Spotipy client credentials flow\n",
    "    _ = require_env(\"SPOTIFY_CLIENT_ID\")\n",
    "    _ = require_env(\"SPOTIFY_CLIENT_SECRET\")\n",
    "\n",
    "print(\"✅ Environment looks OK (or enrichment disabled).\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca020d61-a952-4c45-980a-abdd276d1b3e",
   "metadata": {},
   "source": [
    "## Download Spotify Charts Data\n",
    "\n",
    "Spotify does not provide chart rankings (Daily Top 200 / Weekly Top 50) through the official Spotify Web API. While the API exposes track metadata and audio features, chart positions are not available programmatically.\n",
    "\n",
    "For this reason, we use Kworb, a public and widely used mirror of Spotify charts. Kworb publishes daily and weekly Spotify rankings as HTML tables that can be reliably parsed and are updated shortly after the official Spotify Charts release.\n",
    "\n",
    "The daily endpoint only obtains the daily data from that day, there is no way to obtain the historical.\n",
    "\n",
    "Ingestion process\n",
    "- Build the chart URL for the selected region (daily or weekly).\n",
    "- Download the HTML page from Kworb.\n",
    "- Parse the chart table using pandas.read_html.\n",
    "- Normalize column names and map them to a consistent schema.\n",
    "- Split artist and track names.\n",
    "- Clean numeric fields and attach metadata (date, region).\n",
    "\n",
    "Important Properties:\n",
    "\n",
    "| Field         | Description                           |\n",
    "| ------------- | ------------------------------------- |\n",
    "| `rank`        | Position in the chart (1 = best)      |\n",
    "| `track_name`  | Song title                            |\n",
    "| `artist_name` | Primary artist                        |\n",
    "| `streams`     | Number of streams in the chart period |\n",
    "| `days`        | Days on chart (daily charts)          |\n",
    "| `weeks`       | Weeks on chart (weekly charts)        |\n",
    "| `peak`        | Best historical chart position        |\n",
    "| `region`      | Country code (e.g. `se`)              |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a7a375c2-ae41-431c-b0ae-2fa5508d05bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kworb_url(region: str, frequency: str) -> str:\n",
    "    if region == \"global\":\n",
    "        return f\"https://kworb.net/spotify/{frequency}.html\"\n",
    "    return f\"https://kworb.net/spotify/country/{region}_{frequency}.html\"\n",
    "\n",
    "def _normalize_columns(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    df.columns = [c.strip().lower().replace(\" \", \"_\") for c in df.columns]\n",
    "    return df\n",
    "\n",
    "def _split_artist_track(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    s = df[\"track_raw\"].astype(str)\n",
    "\n",
    "    parts = s.str.split(\" – \", n=1, expand=True)\n",
    "    if parts.shape[1] < 2:\n",
    "        parts = s.str.split(\" - \", n=1, expand=True)\n",
    "\n",
    "    if parts.shape[1] == 2:\n",
    "        df[\"artist_name\"] = parts[0].str.strip()\n",
    "        df[\"track_name\"] = parts[1].str.strip()\n",
    "    else:\n",
    "        df[\"track_name\"] = s\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "60f4bad8-9221-4a2a-9ca6-5b9b9019c6ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_kworb_daily_top200(region: str) -> pd.DataFrame:\n",
    "    url = kworb_url(region, \"daily\")\n",
    "    df = pd.read_html(url)[0]\n",
    "\n",
    "    df = _normalize_columns(df)\n",
    "\n",
    "    df = df.rename(columns={\n",
    "        \"pos\": \"rank\",\n",
    "        \"artist_and_title\": \"track_raw\",\n",
    "        \"days\": \"days\",\n",
    "        \"pk\": \"peak\",\n",
    "        \"streams\": \"streams\",\n",
    "        \"total\": \"total_streams\",\n",
    "    })\n",
    "\n",
    "    df = _split_artist_track(df)\n",
    "    df[\"chart_date\"] = pd.to_datetime(date.today())\n",
    "    df[\"region\"] = region\n",
    "\n",
    "    for c in [\"rank\", \"streams\", \"days\", \"peak\"]:\n",
    "        df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "\n",
    "    df = (\n",
    "        df.dropna(subset=[\"rank\"])\n",
    "          .sort_values(\"rank\")\n",
    "          .head(200)\n",
    "          .reset_index(drop=True)\n",
    "    )\n",
    "\n",
    "    return df[\n",
    "        [\"rank\", \"track_name\", \"artist_name\",\n",
    "         \"streams\", \"days\", \"peak\",\n",
    "         \"chart_date\", \"region\"]\n",
    "    ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3555eb8d-9125-4318-907d-6ff1e84f87a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_kworb_weekly_top50(region: str) -> pd.DataFrame:\n",
    "    url = kworb_url(region, \"weekly\")\n",
    "    df = pd.read_html(url)[0]\n",
    "\n",
    "    df = _normalize_columns(df)\n",
    "\n",
    "    df = df.rename(columns={\n",
    "        \"pos\": \"rank\",\n",
    "        \"artist_and_title\": \"track_raw\",\n",
    "        \"wks\": \"weeks\",\n",
    "        \"pk\": \"peak\",\n",
    "        \"streams\": \"streams\",\n",
    "        \"total\": \"total_streams\",\n",
    "    })\n",
    "\n",
    "    df = _split_artist_track(df)\n",
    "\n",
    "    df[\"week_start\"] = pd.to_datetime(date.today())\n",
    "    df[\"region\"] = region\n",
    "\n",
    "    for c in [\"rank\", \"streams\", \"weeks\", \"peak\"]:\n",
    "        df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "\n",
    "    df = (\n",
    "        df.dropna(subset=[\"rank\"])\n",
    "          .sort_values(\"rank\")\n",
    "          .head(50)\n",
    "          .reset_index(drop=True)\n",
    "    )\n",
    "\n",
    "    return df[\n",
    "        [\"rank\", \"track_name\", \"artist_name\",\n",
    "         \"streams\", \"weeks\", \"peak\",\n",
    "         \"week_start\", \"region\"]\n",
    "    ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4218ae1e-a463-4011-85a6-d3e88aafe820",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Latest daily and weekly data from Spotify\n",
    "df_daily = fetch_kworb_daily_top200(REGION)\n",
    "df_weekly = fetch_kworb_weekly_top50(REGION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c400d19f-5034-498d-b68d-faeba412bb70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique tracks to resolve: 50\n",
      "50\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>track_name</th>\n",
       "      <th>artist_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Last Christmas</td>\n",
       "      <td>Wham!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>All I Want for Christmas Is You</td>\n",
       "      <td>Mariah Carey</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Rockin' Around The Christmas Tree</td>\n",
       "      <td>Brenda Lee</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Tänd ett ljus</td>\n",
       "      <td>Triad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Snowman</td>\n",
       "      <td>Sia</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          track_name   artist_name\n",
       "0                     Last Christmas         Wham!\n",
       "1    All I Want for Christmas Is You  Mariah Carey\n",
       "2  Rockin' Around The Christmas Tree    Brenda Lee\n",
       "3                      Tänd ett ljus         Triad\n",
       "4                            Snowman           Sia"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Universe of tracks (unique per artist + track)\n",
    "df_tracks_universe = (\n",
    "    df_weekly[[\"track_name\", \"artist_name\"]]\n",
    "    .dropna()\n",
    "    .drop_duplicates()\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "print(f\"Unique tracks to resolve: {len(df_tracks_universe)}\")\n",
    "print(len(df_weekly))\n",
    "df_tracks_universe.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32981d64-541f-4a48-8408-8019e24710c6",
   "metadata": {},
   "source": [
    "## Download Youtube Charts Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0a024c7c-fddc-4a65-82df-8dee34860dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import date, timedelta\n",
    "\n",
    "def generate_week_starts(\n",
    "    end_date: date,\n",
    "    n_weeks: int,\n",
    "):\n",
    "    # YouTube Charts weeks start on Friday\n",
    "    # We align to previous Friday\n",
    "    d = end_date\n",
    "    while d.weekday() != 4:  # Friday = 4\n",
    "        d -= timedelta(days=1)\n",
    "\n",
    "    return [d - timedelta(weeks=i) for i in range(n_weeks)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "253f3533-969b-4471-8afb-eda42ba8b566",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "def fetch_youtube_weekly_chart(\n",
    "    country: str,\n",
    "    week_start: date,\n",
    "    top_n: int = 100,\n",
    ") -> pd.DataFrame:\n",
    "\n",
    "    url = (\n",
    "        \"https://charts.youtube.com/api/charts/TopSongs\"\n",
    "        f\"?hl=en&gl={country.upper()}\"\n",
    "        f\"&date={week_start.isoformat()}\"\n",
    "        f\"&limit={top_n}\"\n",
    "    )\n",
    "\n",
    "    r = requests.get(url, timeout=30)\n",
    "    r.raise_for_status()\n",
    "    data = r.json()\n",
    "\n",
    "    print(data)\n",
    "\n",
    "    rows = []\n",
    "    for entry in data.get(\"charts\", [])[0].get(\"entries\", []):\n",
    "        rows.append({\n",
    "            \"rank\": entry[\"rank\"],\n",
    "            \"track_title\": entry[\"title\"],\n",
    "            \"artist_name\": entry[\"artists\"][0][\"name\"] if entry[\"artists\"] else None,\n",
    "            \"video_id\": entry[\"videoId\"],\n",
    "            \"weekly_views\": entry[\"views\"],\n",
    "            \"week_start\": pd.to_datetime(week_start),\n",
    "            \"country\": country.lower(),\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(rows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0566e5ee-3281-4930-a1a8-1d8c04359ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "def backfill_youtube_weekly_charts(\n",
    "    country: str,\n",
    "    n_weeks: int = 12,\n",
    ") -> pd.DataFrame:\n",
    "\n",
    "    weeks = generate_week_starts(date.today(), n_weeks)\n",
    "\n",
    "    all_weeks = []\n",
    "    for w in tqdm(weeks, desc=\"Fetching YouTube weekly charts\"):\n",
    "        try:\n",
    "            df = fetch_youtube_weekly_chart(country, w)\n",
    "            all_weeks.append(df)\n",
    "        except Exception as e:\n",
    "            print(f\"Failed week {w}: {e}\")\n",
    "\n",
    "    return pd.concat(all_weeks, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "052a938f-d7ae-4784-810a-2847f0f641cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42cc8f285b22437a9aeb1ea1007a237d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching YouTube weekly charts:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed week 2025-12-26: Expecting value: line 1 column 1 (char 0)\n",
      "Failed week 2025-12-19: Expecting value: line 1 column 1 (char 0)\n",
      "Failed week 2025-12-12: Expecting value: line 1 column 1 (char 0)\n",
      "Failed week 2025-12-05: Expecting value: line 1 column 1 (char 0)\n",
      "Failed week 2025-11-28: Expecting value: line 1 column 1 (char 0)\n",
      "Failed week 2025-11-21: Expecting value: line 1 column 1 (char 0)\n",
      "Failed week 2025-11-14: Expecting value: line 1 column 1 (char 0)\n",
      "Failed week 2025-11-07: Expecting value: line 1 column 1 (char 0)\n",
      "Failed week 2025-10-31: Expecting value: line 1 column 1 (char 0)\n",
      "Failed week 2025-10-24: Expecting value: line 1 column 1 (char 0)\n",
      "Failed week 2025-10-17: Expecting value: line 1 column 1 (char 0)\n",
      "Failed week 2025-10-10: Expecting value: line 1 column 1 (char 0)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "No objects to concatenate",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[32]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m df_youtube_weekly = \u001b[43mbackfill_youtube_weekly_charts\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcountry\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mse\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m    \u001b[49m\u001b[43mn_weeks\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m12\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m      6\u001b[39m df_youtube_weekly.head()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[31]\u001b[39m\u001b[32m, line 18\u001b[39m, in \u001b[36mbackfill_youtube_weekly_charts\u001b[39m\u001b[34m(country, n_weeks)\u001b[39m\n\u001b[32m     15\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m     16\u001b[39m         \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFailed week \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mw\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mall_weeks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\Documents\\master\\scalable_ML\\ID2223-Final-Project-Spotify\\venv\\Lib\\site-packages\\pandas\\core\\reshape\\concat.py:382\u001b[39m, in \u001b[36mconcat\u001b[39m\u001b[34m(objs, axis, join, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001b[39m\n\u001b[32m    379\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m copy \u001b[38;5;129;01mand\u001b[39;00m using_copy_on_write():\n\u001b[32m    380\u001b[39m     copy = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m382\u001b[39m op = \u001b[43m_Concatenator\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    383\u001b[39m \u001b[43m    \u001b[49m\u001b[43mobjs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    384\u001b[39m \u001b[43m    \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    385\u001b[39m \u001b[43m    \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    386\u001b[39m \u001b[43m    \u001b[49m\u001b[43mjoin\u001b[49m\u001b[43m=\u001b[49m\u001b[43mjoin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    387\u001b[39m \u001b[43m    \u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    388\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlevels\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlevels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    389\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnames\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnames\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    390\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverify_integrity\u001b[49m\u001b[43m=\u001b[49m\u001b[43mverify_integrity\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    391\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    392\u001b[39m \u001b[43m    \u001b[49m\u001b[43msort\u001b[49m\u001b[43m=\u001b[49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    393\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    395\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m op.get_result()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\Documents\\master\\scalable_ML\\ID2223-Final-Project-Spotify\\venv\\Lib\\site-packages\\pandas\\core\\reshape\\concat.py:445\u001b[39m, in \u001b[36m_Concatenator.__init__\u001b[39m\u001b[34m(self, objs, axis, join, keys, levels, names, ignore_index, verify_integrity, copy, sort)\u001b[39m\n\u001b[32m    442\u001b[39m \u001b[38;5;28mself\u001b[39m.verify_integrity = verify_integrity\n\u001b[32m    443\u001b[39m \u001b[38;5;28mself\u001b[39m.copy = copy\n\u001b[32m--> \u001b[39m\u001b[32m445\u001b[39m objs, keys = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_clean_keys_and_objs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobjs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    447\u001b[39m \u001b[38;5;66;03m# figure out what our result ndim is going to be\u001b[39;00m\n\u001b[32m    448\u001b[39m ndims = \u001b[38;5;28mself\u001b[39m._get_ndims(objs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\Documents\\master\\scalable_ML\\ID2223-Final-Project-Spotify\\venv\\Lib\\site-packages\\pandas\\core\\reshape\\concat.py:507\u001b[39m, in \u001b[36m_Concatenator._clean_keys_and_objs\u001b[39m\u001b[34m(self, objs, keys)\u001b[39m\n\u001b[32m    504\u001b[39m     objs_list = \u001b[38;5;28mlist\u001b[39m(objs)\n\u001b[32m    506\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(objs_list) == \u001b[32m0\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m507\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mNo objects to concatenate\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    509\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m keys \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    510\u001b[39m     objs_list = \u001b[38;5;28mlist\u001b[39m(com.not_none(*objs_list))\n",
      "\u001b[31mValueError\u001b[39m: No objects to concatenate"
     ]
    }
   ],
   "source": [
    "df_youtube_weekly = backfill_youtube_weekly_charts(\n",
    "    country=\"se\",\n",
    "    n_weeks=12,\n",
    ")\n",
    "\n",
    "df_youtube_weekly.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ef5783f4-1024-44ad-96d1-0a95fc13af0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://charts.youtube.com/charts/TopSongs/se/weekly/2024-02-09?hl=en&format=csv\n",
      "Status: 200\n",
      "<!DOCTYPE html><html lang=\"en\" dir=\"ltr\"><head><script nonce=\"AhvJ7TIqVFVvXLvw8WeX9w\">if ('undefined' == typeof Symbol || 'undefined' == typeof Symbol.iterator) {delete Array.prototype.entries;}</script><script nonce=\"AhvJ7TIqVFVvXLvw8WeX9w\">var ytcsi={gt:function(n){n=(n||\"\")+\"data_\";return ytcsi[n]||(ytcsi[n]={tick:{},info:{},gel:{preLoggedGelInfos:[]}})},now:window.performance&&window.performance.timing&&window.performance.now&&window.performance.timing.navigationStart?function(){return windo\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from datetime import date\n",
    "\n",
    "test_week = date(2024, 2, 9)  # any Friday\n",
    "\n",
    "url = (\n",
    "    f\"https://charts.youtube.com/charts/TopSongs/se/weekly/\"\n",
    "    f\"{test_week.isoformat()}?hl=en&format=csv\"\n",
    ")\n",
    "\n",
    "print(url)\n",
    "\n",
    "r = requests.get(url, timeout=30)\n",
    "print(\"Status:\", r.status_code)\n",
    "print(r.text[:500])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50156f96",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb407899",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
